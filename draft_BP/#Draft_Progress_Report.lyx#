#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth -2
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Progress Report
\end_layout

\begin_layout Author
BP
\end_layout

\begin_layout Date
date
\end_layout

\begin_layout Section
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
We aim to investigate weather introducing narrow task specific pre-traning
 for the experts before more general MoE traning will increase the weak
 domain specialisation found by fan et al for sequence level routing.
 
\end_layout

\begin_layout Standard
If we can find ways to increase human like domain specialisation for MoE
 experts this would provide transparency into how exactly what recources
 the MoE draws from in order to adress a given prompt.
 This type of visibility would be interesting from an alignment perspective.
\end_layout

\begin_layout Standard
To do this we create a scaled down version of the experiments by fan et
 al showing this weak specailisation.
 We then pretrain the experts and comapre the outcome, with and without
 pretraining.
\end_layout

\begin_layout Subsection
Method
\end_layout

\begin_layout Subsubsection
The original experiment
\end_layout

\begin_layout Standard
The experiment orignally showing weak domain specialisation when trained
 on sequence level data was based on the base architecture of GPT2-mini,
 from the nanoGPT repository with a LoRA extentoin.
 We aim to reproduce a scaled down version of the result in figure2, here
 we observe slightly different activation patterns based on the subject,
 pointing towards weak specialisation (fig2).
 The figure was obtained by:
\end_layout

\begin_layout Enumerate
Training a GPT2-mini based transformer on openWebText a general dataset
 contining lots of information from teh internet.
\end_layout

\begin_layout Enumerate
Giving it subject specific tasks from MMLU - a test dataset continaning
 subject specific questions and answers.
\end_layout

\begin_layout Enumerate
Recording routing behaviour when the MoE was faced with these subject specific
 tasks.
 We have 4 experts in each of our 3 transformer layers.
 This is what we see in the figure.
 (fig2)
\end_layout

\begin_layout Enumerate
For non specialisation we would see equal use of each expert based on subject.
 However here we see some different staple heights suggesting the MoE chooses
 to route differently based on subject.
 This is what hte authors describes as weak specialisatoin.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Graphics
	filename ex2.PNG
	width 60page%

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Fan et al trained on a on a single A100-SXM4-40GB GPU, with the following
 modifications to the nanoGPT architecture.
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="15" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Original Experiment
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Base model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
nanoGPT with LoRA extention
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
dropout
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
leaning rate
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
9.6e-4(min 9.6e-5)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
weight decay
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.5
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
enabled biases
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
for 6k itterations
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
token pass thorugh/ itteration
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1048576
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
gradient accumulation
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
128
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
batch size
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
sequence lenght
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1024
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
total tokens seen by model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6B
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N experts
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tokens / parameter
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
20 (chincilla)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
optioanl load balancing loss (experts)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
weight 
\begin_inset Formula $\lambda=0.01$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tokensizer
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
openai gpt2
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
For our specific figure a top k=2 level routing was used, for the sequence
 level routing the softmax function was applied twise as below:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p_{i}(x)=\frac{e^{h_{i}(x)}}{\sum_{j}e^{h_{j}(x)}},y=\sum_{i=\tau}\frac{e^{p_{i}(x)}}{\sum_{j\in\tau}e^{p_{j}(x)}}E_{i}(x)
\]

\end_inset


\end_layout

\begin_layout Standard
Here:
\end_layout

\begin_layout Itemize
x - is the input token or sequence embedding
\end_layout

\begin_layout Itemize
\begin_inset Formula $\tau$
\end_inset

 is the set of top-k indices (K=2)
\end_layout

\begin_layout Itemize
\begin_inset Formula $p_{i}(x)$
\end_inset

 are the logits produced by the gating network as shown
\end_layout

\begin_layout Itemize
\begin_inset Formula $E_{i}(x)$
\end_inset

 is the output of the i-th expert
\end_layout

\begin_layout Itemize
\begin_inset Formula $y$
\end_inset

 is the overall output, which is a weighted sum of the two selected exerpts.
 
\end_layout

\begin_layout Subsubsection
Asymptotics of Routing Behaviour
\end_layout

\begin_layout Standard
Fan et al trained on a on a single A100-SXM4-40GB GPU.
 As we have a limit of 16GB vram this poses a harware constraint for our
 experiement, however note that we are not nesessarily looking to create
 a great lanauge model as evaluated on MMLU, but rather to observe routing
 behaviour when provided a subject specific task.
 Furhter experiments by fan et al shows that routing behaviour seems to
 stabilise quite early in the traning process.
 Especially for sequence level top 2 routing and top 3 routing when load
 balancing is applied.
 Fig 2 shows routing decitions during traning on open text web, in each
 figure there are 4 points one for each expert at any given traning itteration,
 if no specialisation was happening we would expect all experts to be used
 equally much.
 Or maybe some expert to be used a lot and some not at all.
 The task they are traning on is likely next token prediction, on open web
 text, for which we can imagine each expert learning some subtask taht is
 needed some given percent of the time, as seen on the y axis.
 When an expert learns a task that is used some given percent of the time
 we see the use on average converge to a line for that percentage, read
 y axis.
\end_layout

\begin_layout Standard
Based on this figure we see convergence quite quickly and hence we suggest
 that a smaller scale experiment with less traning itterations may still
 provide good insight into how routing behaviour differs with and without
 domain specific expert pretraning.
 The archietcture trained on, GPT2-small, uses 12 transformer blocks with
 the FFN layer in each block here changed out for a MoE instead with 4 experts.
 Hence we see 12 figures with 4 dots, one for each expert at any given traning
 itteration.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Graphics
	filename ex2_1.PNG
	width 60page%

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Scaling down Fan et al
\end_layout

\begin_layout Standard
For our experiment we propose a smaller version of fan et al to attempt
 reproducing weak topic specialisation for sequence level routing.
 Traning on a subset of openWebText to be able to run on a 16GB vram card.
\end_layout

\begin_layout Standard
We start from the same basic nanoGpt architecture based on Daniel Grittners
 repository (), we add alongside the basic MLP class a SequenceMoE class
 used when the number of experts is set to more than 0.
 This class contains, just like for the MLP, a forward function for the
 MoE, which also handles load balancing with a paramter 
\begin_inset Formula $\lambda$
\end_inset

.
 We saw in figure 2 that the load balancing had a possitive effect on making
 sure experts stabilise early, which is a useful assumption for scaling
 down this experiment.
 Load balancing mainly aims to mitigate expert collapse.
 Adding loadbalancing we update our loss as follows: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L_{total}=L_{LM}+\lambda L_{aux}
\]

\end_inset


\end_layout

\begin_layout Standard
The origianl language modelling loss, cross entropy term 
\begin_inset Formula $L_{LM}$
\end_inset

 is meassureing how well teh model predict the next token in a sequence
 given the proceeding tokens, where 
\begin_inset Formula $P(y_{t}\mid x_{1:t-1})$
\end_inset

 is the probability assigned by the model to the corret next token 
\begin_inset Formula $y_{t}$
\end_inset

, and 
\begin_inset Formula $T$
\end_inset

 is the total number of tokens being predicted, the sequence lenght, we
 find it in the GPT class Forward function as: main_loss = F.cross_entropy(logits.
view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1), where F is
 import torch.nn.functional as F:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L_{LM}=-\frac{1}{T}\sum_{t=1}^{T}log(P(y_{t}\mid x_{1:t-1}))
\]

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $L_{aux}$
\end_inset

 is a product of expert importance/traffic (P) and expert load/frequency
 of selection (F) summed over all experts (N):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L_{aux}=N\sum_{i=1}^{N}P_{i}F_{i}
\]

\end_inset


\end_layout

\begin_layout Standard
Expert importance Pi, is the expected fraction of traffic routed to expert
 i.
 The average of the softmax probabilities pi(x) assgined to expert i across
 the entire batch B.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P_{i}=\frac{1}{\mid B\mid}\sum_{x\in B}p_{i}(x)
\]

\end_inset


\end_layout

\begin_layout Standard
Expert Load (Fi) meassures the actual fraction of sequnces that selected
 expert i.
 It is the average ofa binarty indivator funciton 
\begin_inset Formula $(1_{selected})$
\end_inset

 across teh batch B, whih is 1 if expert i was one of the top K chosen experts
 for sequence x and 0 otherwise.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
F_{i}=\frac{1}{\mid B\mid}\sum_{x\in B}1_{selected}(x,i)
\]

\end_inset


\end_layout

\begin_layout Standard
In our function SequenceMoE.forward we have Pi as expert_importance, Fi as
 expert_load, and Laux as load_balancing_loss, and Ltotal as total_loss
\end_layout

\begin_layout Subsubsection
The modified Forward function:
\end_layout

\begin_layout Itemize
We feed in a batch x, which contains B samples of T tokens with embedding
 dimention C
\end_layout

\begin_layout Itemize
We then average over the dimention T, to get a batch of instead of sequences
 just one average token per sequence, so B average tokens of embedding dim
 C.
\end_layout

\begin_layout Itemize
Our router assignes weights based on these average tokens.
 We get these by simply passing it through our onle dense FFN router layer
 to obtain the logits, which we then normalise with softmax to obtain the
 routing probabilites for each average token.
 The paper mentions a 
\begin_inset Quotes eld
\end_inset

double softmax
\begin_inset Quotes erd
\end_inset

 this is the first part of it.
\end_layout

\begin_layout Itemize
We use the topk funciton to obtain the selected top k experts and their
 routing weighting/probability we obtained in step 3.
 We now normalise this routing probability over the k selected experts so
 it sums to 1 for teh selected k experts excluding the non selected ones.
 This is the second part of the 
\begin_inset Quotes eld
\end_inset

double softmax
\begin_inset Quotes erd
\end_inset

 described.
\end_layout

\begin_layout Itemize
We then loop through the batch B times, and obtain, this time the acutal
 sequence.
 (T,C)
\end_layout

\begin_deeper
\begin_layout Itemize
We then in an inner loop loop through each of our selected experts from
 earlier based on our average tokens but now we route the actual sequences.
 
\end_layout

\begin_layout Itemize
For each sequence we then have a weighted average between the output from
 the top k experts.
 (output_i)
\end_layout

\end_deeper
\begin_layout Itemize
We gather all the wieghted average logits from passing the sequences through
 the expert FFNs, (all output_is) into a final output matrix, this is the
 updated x vector after passing it through the MoE layer.
 This vector is later passed through a language modelling head (see the
 GPT class), linear layer to proejct the x vector from the embedding dimention
 C upto the vocabulary size.
 We then take the softmax over this final vector and choose the next token
 with the highest probability.
\end_layout

\begin_layout Subsubsection
Optimised forward function
\end_layout

\begin_layout Standard
However we found that this setup was very slow to run comapred to the regular
 GPT2 traning on openWebText as suggested by Karpathy (nanoGPT, repo TODO).
 One reason beeing the introduction of the double loop through B sequences
 and for every sequence k experts.
 Hene B*k expert FFN forward passes per batch x, instead of just one FFN
 forward pass in Kaparthys example.
 We optimise by replacing the double loop with a smaller loop over the K
 experts, 
\end_layout

\end_body
\end_document
